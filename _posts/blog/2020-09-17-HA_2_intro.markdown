---
layout:     post
title:      "스토리지 이중화 2편: 스토리지 이중화 아키텍처 설계"
date:       2021-01-14
author:     김 태훈 (thkim@gluesys.com)
categories: blog
tags:       HA-Pacemaker-이중화-NAS-스토리지-HDD-SSD.
cover:      "/assets/HA1_maincover.jpg"
main:       "/assets/HA1_maincover.jpg"
---

안녕하세요. 지난번 포스팅 [고가용성과 이중화](https://tech.gluesys.com/blog/2020/08/22/HA_1_intro.html)에 이어서 이번 포스트에서는 클라이언트 환경에 따른 스토리지 이중화 아키텍처 설계 방법과 실 구축 사례를 소개하겠습니다.

&nbsp;

## NAS HA Architecture

NAS(Network Attached Storage)는 네트워크 기반의 파일 공유 서비스를 수행하는 스토리지이며, 클라이언트는 일반적으로 이더넷 네트워크를 통해 NAS에 접속하여 파일을 저장하거나 불러올 수 있습니다.

이를 위해 NAS는 스토리지 본연의 기능인 볼륨 구성 및 데이터 저장, 백업 등의 기술만이 아닌, 계정 관리와 인증, 파일 공유, 네트워크, 보안 등, 클라이언트 환경에 대한 인프라 기능을 제공해야 합니다.

더 나아가 단일 스토리지가 아닌 **NAS 이중화 환경**으로 구성할 경우, 이중화를 위한 부수적인 기능들이 추가되어야 합니다. 이때 가장 중요한 요소는 **구축하고자 하는 목적과 클라이언트 인프라에 대한 이해**입니다. 지난 포스트에서 설명했듯이 잘못된 지점을 모니터링하거나 중요한 지점을 관리하지 못한다면 예상치 못한 장애로 failover가 발생할 수 있습니다.

따라서 NAS 이중화는 스토리지에 대한 기능과 파일 공유를 위한 다양한 요소들을 빠짐없이 모니터링해야하기 때문에 다른 이중화 서비스에 비교해서 보다 높은 기술 지식과 환경에 대한 이해를 요구합니다.

&nbsp;

## NAS 이중화 사례 설명

그렇다면 실제 구축된 사례를 예시로 NAS 이중화 아키텍처를 설계하는 과정을 설명해보겠습니다.

&nbsp;

![Alt text](/assets/HA2_FIG1.jpg){: width="500"}
<center>&#60;VDI 환경을 위한 NAS 이중화 시스템 구성도&#62;</center>

&nbsp;

위 그림은 ㅎ사에 구축된 업무 환경을 위한 VDI(Virtual Desktop Infrastructure)[^1] 기반의 시스템 구성도이며, 고객사에서 요구하는 기능은 다음과 같습니다.

> * VDI 클러스터는 가상머신의 이미지를 저장할 용도로 NFS[^2] 공유 환경을 제공받아야 함
> * 서비스 도중에 failover가 발생하더라도 실행 중인 가상머신에 영향이 없어야 함
> * 가상머신에 설치된 운영체제는 windows 기반이며, CIFS(Common Interest File System)[^3] 공유 환경을 제공받아야 함
> * 계정 정보는 AD(Active Directory)[^4]로 통합 관리되며, NAS는 AD와 연동하여 사용자 인증을 처리해야 함
> * 동시 접속자는 약 1000명이며, 1000대의 가상머신과 파일 공유 환경을 동시에 제공해야 함

ㅎ사의 환경은 조금 특별했습니다. 고객사는 기존에 가상화 기술과 스토리지가 연동된 HCI(Hyper-Converged Infrastructure)[^5] 장비를 통해 가상머신의 이미지를 클러스터 스토리지에서 관리하고, 가상머신으로 NAS를 구축하여 사용자 파일을 공유하고 있었습니다. 하지만 잦은 장애와 부족한 NAS 성능으로 원활한 업무 환경 제공에 많은 노력이 필요했다고 합니다.

이번 VDI 구축 사업에는 인프라 구성에 필요한 가상머신 이미지와 개인 사용자 파일을 별도의 고성능 스토리지에 저장하고 공유하기를 원했습니다. 이 경우 NAS에 문제가 발생한다면 사용자 파일 공유뿐만 아니라, 인프라 전체가 다운될 수 있어서 NAS 이중화 환경 구성은 필수였습니다. 만약 장애가 발생하여 standby 스토리지에 서비스가 이전(failover) 되더라도 실행 중인 가상머신에는 영향이 없어야 하기 때문에 런타임 서비스 이전(runtime failover)이 가능한 고성능 NAS 이중화 기술이 필요했습니다.

&nbsp;

![Alt text](/assets/HA2_FIG4.jpg){: width="500"}
<center>&#60;(좌)VDI 환경의 입출력 패턴 / (우)평균 IOPS 기준에 따른 성능 체감 기준[^6]&#62;</center>

&nbsp;

가상화 환경은 real-time small data 패턴의 입출력 요청(I/O request)이 발생합니다. ㅎ사는 별도의 사용자 데이터가 저장될 공간을 가상머신 이미지로부터 분리했기 때문에 가상머신의 대부분의 입출력 요청은 사용자 파일을 위한 요청이 아닌 게스트 운영체제에서 요청한 것이며, 그 단위는 작게는 byte에서 KB 정도의 크기입니다. 이번 구축 사례에서는 ㅎ사의 입출력 패턴에 최적화하기 위해 기존의 파일시스템 기반의 클러스터링 기술이 아닌 블록 기반의 데이터 미러링 기술을 사용했습니다. 클러스터 파일시스템의 공유 단위가 파일이라면, 블록 기반의 데이터 미러링은 실제 디스크에 저장되는 블록 단위로 저장되는 차이점이 있습니다.

&nbsp;

![Alt text](/assets/HA2_FIG2.jpg){: width="500"}
<center>&#60;블록 기반의 데이터 미러링 기술의 입출력 흐름도&#62;</center>

&nbsp;

위 그림은 블록 기반의 데이터 미러링 기술의 입출력 흐름도입니다. 스토리지는 active 노드에서 NAS 서비스를 수행하고 있으며, 클라이언트의 파일 공유 요청은 CIFS와 NFS와 같은 공유 프로토콜과 다양한 레이어를 지나 소프트웨어 계층의 최하단인 데이터 미러링 스택에 도착합니다(1). 데이터 미러링에 전달된 블록 입출력 요청은 active 노드의 디스크에서 처리하는 동시에 미러링 된 standby 노드에도 전달합니다(2). Standby 노드의 데이터 미러링 스택은 로컬 디스크에도 동일한 블록 입출력 요청을 처리하고 그 결과를 active 노드에 전달합니다(4). Active 노드의 데이터 미러링 스택은 입출력 결과를 상위 계층에 전달하며(5) 입출력 요청을 완료합니다.

데이터 미러링 기술에서 가장 중요한 이슈는 성능과 안정성입니다. 먼저 이중화된 노드 사이에 데이터 미러링에 소요되는 지연시간(latency)을 최소화하기 위해 스토리지 간의 통신 인피니밴드(infiniband)를 사용했습니다. 그리고 약 1000명의 VM과 사용자로부터 발생하는 random access 패턴[^7]에 대한 소요되는 시간을 최소하기 위해 검색 시간(seek time)이 낮은 SSD로 디스크를 구성하여 저지연성을 확보했습니다. 이때 시스템 내부의 장애 상황을 대비하여 인피니밴드는 본딩(bonding)[^8] 기술을 통해 이중화로 구성했으며, SSD는 RAID-10로 구성하여 IOPS(Input/Ouput Operations Per Second)를 높이고 패리티 연산으로 소모되는 플래시 메모리 수명을 최소화하여 성능과 안정성을 확보했습니다.

데이터 미러링 기술에는 리눅스에서 블록 기반의 데이터 이중화 기술로 인정받은 DRBD(Distributed Replicasted Storage System)[^9]를 사용합니다(DRBD에 대한 설명은 다른 포스트에서 자세히 다루도록 하겠습니다). DRBD에서 노드 간의 통신은 앞서 설명한 바와 같이 인피니밴드를 이용하여 지연시간을 최소화하였으며, 확보된 성능을 활용하여 모든 입출력 요청이 완료된 이후에 완료 응답을 송신하는 DRBD Protocol C 타입으로 미러링 했습니다.

&nbsp;

![Alt text](/assets/HA2_FIG3.jpg){: width="500"}
<center>&#60;ㅎ사에 구축된 NAS 이중화 아키텍처&#62;</center>

&nbsp;

다음으로 고성능 NAS 이중화 구성에 필요한 모니터링 요소와 관계에 대해서 설명하겠습니다. 위 구조도는 ㅎ사에 구축된 NAS 이중화 시스템의 모니터링 아키텍처 입니다. 모니터링 요소 간의 배치는 입출력 요청에 대한 처리 순서와 요소들 간의 관계에 따라서 결정되며, 각 모니터링 요소에 대한 설명은 아래와 같습니다.

> 1. Virtual IP - 클라이언트가 접속하고자 하는 서비스 IP 할당
> 2. Ethernet Interface - Virtual IP가 할당된 네트워크 인터페이스의 상태 확인
> 3. AD Connector - SMB와 NFS에서 사용할 계정 인증 커넥터
> 4. NFS or SMB - 파일 공유 서비스의 상태 확인
> 5. Filesystem - 파일시스템 R/W 가능 여부 확인
> 6. LVM[^10] - 볼륨 그룹 및 그룹에 속하는 블록 장치의 상태 확인
> 7. DRBD - 데이터 미러링을 수행하는 클러스터 데몬 상태 확인

&nbsp;

![Alt text](/assets/HA2_FIG5.jpg){: width="500"}
<center>&#60;잘못된 아키텍처 설계 예시&#62;</center>

&nbsp;

이때 주의해야 할 부분은 모니터링 요소의 배치에 따라 서비스의 연속성 보장에 실패할 수 있습니다. 한 예로 클라이언트가 접속할 네트워크 주소를 할당하는 Virtual IP 에이전트가 파일 공유 프로토콜 스택인 NFS와 SMB보다 아래에 있다면 failover 발생 시 클라이언트 접속이 끊기게 됩니다. 클라이언트 입장에서 두 구조에 따른 차이는 다음과 같습니다.

> - 구조 A (정상)
>   > IP가 할당된 시점에 이미 파일 공유 데몬이 준비되었기 때문에 통신이 재개된 후에 입출력 요청 큐에 대기 중인 작업들도 모두 데몬과 통신하여 처리됨
> - 구조 B (비정상)
>   > IP가 할당된 시점에 파일 공유 데몬이 준비되지 않아 통신이 재개된 클라이언트들의 입출력 요청을 처리하지 못함

위 예시처럼 NAS 이중화 아키텍처 설계에서 가장 중요한 부분은 클라이언트 환경에 대한 이해와 기술 간의 관계를 고려하는 것입니다.

&nbsp;

## 마치며

이번 포스트에서는 고객 사례를 예시로 NAS 이중화 아키텍처에 대해서 설명했습니다. 다음 포스트는 미디어 환경에서 active/active 방식의 NAS 이중화 구성 사례를 설명해드리도록 하겠습니다. 또 봐요 ~ :wave:

&nbsp;

## 참고

 * https://www.netapp.com/media/19785-wp-sizing-storage-for-desktop.pdf
 * https://www.infortrend.com/ImageLoader/LoadDoc/509/True/True/Infortrend%20document

## 각주

[^1]: https://wikipedia.org/wiki/Desktop_virtualization
[^2]: https://wikipedia.org/wiki/Network_File_System
[^3]: https://wikipedia.org/wiki/Server_Message_Block
[^4]: https://wikipedia.org/wiki/Active_Directory
[^5]: https://wikipedia.org/wiki/Hyper-converged_infrastructure
[^6]: https://www.netapp.com/media/19785-wp-sizing-storage-for-desktop.pdf
[^7]: https://www.infortrend.com/ImageLoader/LoadDoc/509/True/True/Infortrend%20document
[^8]: https://wikipedia.org/wiki/Link_aggregation
[^9]: https://wikipedia.org/wiki/Distributed_Replicated_Block_Device
[^10]: https://wikipedia.org/wiki/Logical_Volume_Manager_(Linux)
